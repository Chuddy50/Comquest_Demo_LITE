{
  "title": "Things Zig comptime won't do",
  "by": "JadedBlueEyes",
  "url": "https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html",
  "blocks": [
    {
      "block": [
        {
          "id": 43745438,
          "author": "pron",
          "text": "Yes! To me, the uniqueness of Zig&#x27;s comptime is a combination of two things: 1. comtpime <i>replaces</i> many other features that would be specialised in other languages with or without rich compile-time (or runtime) metaprogramming, <i>and</i> 2. comptime is referentially transparent [1], that makes it strictly &quot;weaker&quot; than AST macros, but simpler to understand; what&#x27;s surprising is just how capable you can be with a comptime mechanism with access to introspection yet without the referentially opaque power of macros. These two give Zig a unique combination of simplicity and power. We&#x27;re used to seeing things like that in Scheme and other Lisps, but the approach in Zig is very different. The outcome isn&#x27;t as general as in Lisp, but it&#x27;s powerful enough while keeping code easier to understand. You can like it or not, but it is very interesting and very novel (the novelty isn&#x27;t in the feature itself, but in the place it has in the language). Languages with a novel design and approach that you can learn in a couple of days are quite rare. [1]: In short, this means that you get no access to names or expressions, only the values they yield.",
          "depth": 0,
          "sentiment": 0.1926470588235294
        },
        {
          "id": 43745928,
          "author": "paldepind2",
          "text": "I was a bit confused by the remark that comptime is referentially transparent. I&#x27;m familiar with the term as it&#x27;s used in functional programming to mean that an expression can be replaced by its value (stemming from it having no side-effects).  However, from a quick search I found an old related comment by you [1] that clarified this for me. If I understand correctly you&#x27;re using the term in a different (perhaps more correct&#x2F;original?) sense where it roughly means that two expressions with the same meaning&#x2F;denotation can be substituted for each other without changing the meaning&#x2F;denotation of the surrounding program. This property is broken by macros. A macro in Rust, for instance, can distinguish between `1 + 1` and `2`. The comptime system in Zig in contrast does not break this property as it only allows one to inspect values and not un-evaluated ASTs. [1]: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36154447\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36154447</a>",
          "depth": 1,
          "sentiment": -0.0022435897435897443
        },
        {
          "id": 43746707,
          "author": "pron",
          "text": "Yes, I am using the term more correctly (or at least more generally), although the way it&#x27;s used in functional programming is a special case. A referentially transparent term is one whose sub-terms can be replaced by their references without changing the reference of the term as a whole. A functional programming language is simply one where all references are values or &quot;objects&quot; <i>in the programming language itself</i>. The expression `i++` in C is not a value <i>in C</i> (although it is a &quot;value&quot; in some semantic descriptions of C), yet a C expression that contains `i++` and cannot distinguish between `i++` and any other C operation that increments i by 1, is referentially transparent, which is pretty much all C expressions except for those involving C macros. Macros are not referentially transparent because they can distinguish between, say, a variable whose <i>name</i> is `foo` and is equal to 3 and a variable whose name is `bar` and is equal to 3. In other words, their outcome may differ not just by what is being referenced (3) but also by <i>how</i> it&#x27;s referenced (`foo` or `bar`), hence they&#x27;re referentially opaque.",
          "depth": 2,
          "sentiment": 0.11593406593406594
        },
        {
          "id": 43746591,
          "author": "deredede",
          "text": "Those are equivalent, I think. If you can replace an expression by its value, any two expressions with the same value are indistinguishable (and conversely a value is an expression which is its own value).",
          "depth": 2,
          "sentiment": 0.3
        },
        {
          "id": 43747250,
          "author": "WalterBright",
          "text": "It&#x27;s not novel. D pioneered compile time function execution (CTFE) back around 2007. The idea has since been adopted in many other languages, like C++. One thing it is used for is generating string literals, which then can be fed to the compiler. This takes the place of macros. CTFE is one of D&#x27;s most popular and loved features.",
          "depth": 1,
          "sentiment": 0.3625
        },
        {
          "id": 43747875,
          "author": "msteffen",
          "text": "If I understand TFA correctly, the author claims that D\u2019s approach is actually different: <a href=\"https:&#x2F;&#x2F;matklad.github.io&#x2F;2025&#x2F;04&#x2F;19&#x2F;things-zig-comptime-wont-do.html#:~:text=For%20example%2C%20D%20mixins%20work%20that%20way\" rel=\"nofollow\">https:&#x2F;&#x2F;matklad.github.io&#x2F;2025&#x2F;04&#x2F;19&#x2F;things-zig-comptime-won...</a> \u201cIn contrast, there\u2019s absolutely no facility for dynamic source code generation in Zig. You just can\u2019t do that, the feature isn\u2019t! [sic] Zig has a completely different feature, partial evaluation&#x2F;specialization, which, none the less, is enough to cover most of use-cases for dynamic code generation.\u201d",
          "depth": 2,
          "sentiment": 0.014814814814814812
        },
        {
          "id": 43748490,
          "author": "WalterBright",
          "text": "The partial evaluation&#x2F;specialization is accomplished in D using a template. The example from the link: <pre><code>    fn f(comptime x: u32, y: u32) u32 {\n        if (x == 0) return y + 1;\n        if (x == 1) return y * 2;\n        return y;\n    }\n</code></pre>\nand in D: <pre><code>    uint f(uint x)(uint y) {\n        if (x == 0) return y + 1;\n        if (x == 1) return y * 2;\n        return y;\n    }\n</code></pre>\nThe two parameter lists make it a function template, the first set of parameters are the template parameters, which are compile time. The second set are the runtime parameters. The compile time parameters can also be types, and aliased symbols.",
          "depth": 3,
          "sentiment": 0.0875
        },
        {
          "id": 43747836,
          "author": "az09mugen",
          "text": "A little bit out of context, I just want to thank you and all the contributors for the D programming language.",
          "depth": 2,
          "sentiment": -0.1875
        },
        {
          "id": 43746682,
          "author": "cannabis_sam",
          "text": "Regarding 2. How are comptime values restricted to total computations? Is it just by the fact that the compiler actually finished, or are there any restrictions on comptime evaluations?",
          "depth": 1,
          "sentiment": 0.0
        },
        {
          "id": 43746763,
          "author": "pron",
          "text": "They don&#x27;t need to be restricted to total computation to be referentially transparent. Non-termination is also a reference.",
          "depth": 2,
          "sentiment": 0.0
        },
        {
          "id": 43745704,
          "author": "User23",
          "text": "Has anyone grafted Zig style macros into Common Lisp?",
          "depth": 1,
          "sentiment": -0.3
        },
        {
          "id": 43746089,
          "author": "Conscat",
          "text": "The Scopes language might be similar to what you&#x27;re asking about. Its notion of &quot;spices&quot; which complement the &quot;sugars&quot; feature is a similar kind of constant evaluation. It&#x27;s not a Common Lisp dialect, though, but it is sexp based.",
          "depth": 2,
          "sentiment": 0.15
        },
        {
          "id": 43745832,
          "author": "toxik",
          "text": "Isn\u2019t this kind of thing sort of the default thing in Lisp? Code is data so you can transform it.",
          "depth": 2,
          "sentiment": 0.6
        },
        {
          "id": 43746555,
          "author": "fn-mote",
          "text": "There are no limitations on the transformations in lisp. That can make macros very hard to understand. And hard for later program transformers to deal with. The innovation in Zig is the restrictions that limit the power of macros.",
          "depth": 3,
          "sentiment": -0.22361111111111112
        },
        {
          "id": 43747714,
          "author": "TinkersW",
          "text": "Lisp is so powerful, but without static types you can&#x27;t even do basic stuff like overloading, and have to invent a way to even check the type(for custom types) so you can branch on type.",
          "depth": 3,
          "sentiment": 0.26666666666666666
        },
        {
          "id": 43748318,
          "author": "wild_egg",
          "text": "&gt; but without static types So add static types. <a href=\"https:&#x2F;&#x2F;github.com&#x2F;coalton-lang&#x2F;coalton\">https:&#x2F;&#x2F;github.com&#x2F;coalton-lang&#x2F;coalton</a>",
          "depth": 4,
          "sentiment": 0.5
        },
        {
          "id": 43747885,
          "author": "dokyun",
          "text": "&gt; Lisp is so powerful, but &lt;tired old shit from someone who&#x27;s never used Lisp&gt;. You use defmethod for overloading. Types check themselves.",
          "depth": 4,
          "sentiment": 0.06666666666666667
        },
        {
          "id": 43745860,
          "author": "Zambyte",
          "text": "There isn&#x27;t really as clear of a distinction between &quot;runtime&quot; and &quot;compile time&quot; in Lisp. The comptime keyword is essentially just the opposite of quote in Lisp. Instead of using comptime to say what should be evaluated early, you use quote to say what should be evaluated later. Adding comptime to Lisp would be weird (though obviously not impossible, because it&#x27;s Lisp), because that is essentially the default for expressions.",
          "depth": 2,
          "sentiment": 0.004166666666666666
        },
        {
          "id": 43746099,
          "author": "Conscat",
          "text": "The truth of this varies between Lisp based languages.",
          "depth": 3,
          "sentiment": 0.0
        },
        {
          "id": 43747113,
          "author": "keybored",
          "text": "I\u2019ve never managed to understand your year-long[1] manic praise over this feature. Given that you\u2019re a language implementer. It\u2019s very cool to be able to just say \u201cY is just X\u201d.  You know in a museum.  Or at a distance.  Not necessarily as something you have to work with daily.  Because I would rather take something ranging from Java\u2019s interface to Haskell\u2019s typeclasses since once implemented, they\u2019ll just work.  With comptime types, according to what I\u2019ve read, you\u2019ll have to bring your T to the comptime and find out right then and there if it will work.  Without enough foresight it might not. That\u2019s not something I want.  I just want generics or parametric polymorphism or whatever it is to work once it compiles.  If there\u2019s a &lt;T&gt; I want to slot in T without any surprises.  And whether Y is just X is a very distant priority at that point.  Another distant priority is if generics and whatever else <i>is all just X undernea</i>... I mean just let me use the language declaratively. I felt like I was on the idealistic end of the spectrum when I saw you criticizing other languages that are not installed on 3 billion devices as too academic.[2]  Now I\u2019m not so sure? [1] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24292760\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24292760</a> [2] But does Scala technically count since it\u2019s on the JVM though?",
          "depth": 1,
          "sentiment": 0.08943452380952381
        },
        {
          "id": 43748238,
          "author": "hitekker",
          "text": "Do you have a source for &quot;criticizing other languages not installed on 3 billion devices as too academic&quot; ? Without more context, this comment sounds like rehashing old (personal?) drama.",
          "depth": 2,
          "sentiment": 0.11875
        },
        {
          "id": 43747748,
          "author": "ww520",
          "text": "I&#x27;m sorry but I don&#x27;t understand what you&#x27;re complaining about comptime. All the stuff you said you wanted to work (generic, parametric polymorphism, slotting &lt;T&gt;, etc) just work with comptime.  People are praising about comptime because it&#x27;s a simple mechanism that replacing many features in other languages that require separate language features.  Comptime is very simple and natural to use.  It can just float with your day to day programming without much fuss.",
          "depth": 2,
          "sentiment": 0.021875000000000002
        }
      ],
      "avg_sentiment": 0.09898185284214697
    },
    {
      "block": [
        {
          "id": 43745091,
          "author": "hiccuphippo",
          "text": "The quote in Spanish about a Norse god is from a story by Jorge Luis Borges, here&#x27;s an English translation: <a href=\"https:&#x2F;&#x2F;biblioklept.org&#x2F;2019&#x2F;04&#x2F;02&#x2F;the-disk-a-very-short-story-by-jorge-luis-borges&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;biblioklept.org&#x2F;2019&#x2F;04&#x2F;02&#x2F;the-disk-a-very-short-sto...</a>",
          "depth": 0,
          "sentiment": 0.0
        },
        {
          "id": 43746245,
          "author": "kruuuder",
          "text": "If you have read the story and, like me, are still wondering which part of the story is the quote at the top of the post: &quot;It&#x27;s Odin&#x27;s Disc. It has only one side. Nothing else on Earth has only one side.&quot;",
          "depth": 1,
          "sentiment": 0.16666666666666666
        },
        {
          "id": 43747092,
          "author": "tines",
          "text": "A mobius strip does!",
          "depth": 2,
          "sentiment": 0.0
        },
        {
          "id": 43745676,
          "author": "_emacsomancer_",
          "text": "And in Spanish here: <a href=\"https:&#x2F;&#x2F;www.poeticous.com&#x2F;borges&#x2F;el-disco?locale=es\" rel=\"nofollow\">https:&#x2F;&#x2F;www.poeticous.com&#x2F;borges&#x2F;el-disco?locale=es</a> (Not having much Spanish, I at first thought &quot;Odin&#x27;s disco(teque)&quot; and then &quot;no, that doesn&#x27;t make sense about sides&quot;, but then, surely primed by English &quot;disco&quot;, thought &quot;it must mean Odin&#x27;s record&#x2F;lp&#x2F;album&quot;.)",
          "depth": 1,
          "sentiment": 0.07291666666666667
        },
        {
          "id": 43745774,
          "author": "wiml",
          "text": "Odin&#x27;s records have no B-sides, because everything Odin writes is fire!",
          "depth": 2,
          "sentiment": 0.0
        },
        {
          "id": 43746803,
          "author": "tialaramex",
          "text": "Back when things really had A and B sides, it was moderately common for big artists to release a &quot;Double A&quot; in which both titles were heavily promoted, e.g. Nirvana&#x27;s &quot;All Apologies&quot; and &quot;Rape Me&quot; are a double A, the Beatles &quot;Penny Lane&quot; and &quot;Strawberry Fields Forever&quot; likewise.",
          "depth": 3,
          "sentiment": -0.049999999999999996
        },
        {
          "id": 43747689,
          "author": "Validark",
          "text": "The story is indeed very short, but hits hard. Odin reveals himself and his mystical disc that he states makes him king as long as he holds it. The Christian hermit (by circumstance) who had previously received him told him he didn&#x27;t worship Him, that he worshiped Christ instead, and then murdered him for the disc in the hopes he could sell it for a bunch of money. He dumped Odin&#x27;s body in the river and never found the disc. The man hated Odin to this day for not just handing over the disc to him. I wonder if there&#x27;s some message in here. As a modern American reader, if I believed the story was contemporary, I&#x27;d think it&#x27;s making a point about Christianity substituting honor for destructive greed. That a descendant of the wolves of Odin would worship a Hebrew instead and kill him for a bit of money is quite sad, but I don&#x27;t think it an inaccurate characterization. There&#x27;s also the element of resentment towards Odin for not just handing over monetary blessings. That&#x27;s sad to me as well. Part of me hopes that one day Odin isn&#x27;t held in such contempt.",
          "depth": 1,
          "sentiment": -0.2032051282051282
        }
      ],
      "avg_sentiment": -0.001945970695970697
    },
    {
      "block": [
        {
          "id": 43745670,
          "author": "ephaeton",
          "text": "zig&#x27;s comptime has some (objectively: debatable? subjectively: definite) shortcomings that the zig community then overcomes with zig build to generate code-as-strings to be lateron @imported and compiled. Practically, &quot;zig build&quot;-time-eval. As such there&#x27;s another &#x27;comptime&#x27; stage with more freedom, unlimited run-time (no @setEvalBranchQuota), can do IO (DB schema, network lookups, etc.) but you lose the freedom to generate zig types as values in the current compilation; instead of that you of course have the freedom to reduce-&gt;project from target compiled semantic back to input syntax down to string to enter your future compilation context again. Back in the day, where I had to glue perl and tcl via C at one point in time, passing strings for perl generated through tcl is what this whole thing reminds me of. Sure it works. I&#x27;m not happy about it. There&#x27;s _another_ &quot;macro&quot; stage that you can&#x27;t even see in your code (it&#x27;s just @import). The zig community bewilders me at times with their love for lashing themselves. The sort of discussions which new sort of self-harm they&#x27;d love to enforce on everybody is borderline disturbing.",
          "depth": 0,
          "sentiment": 0.08005050505050504
        },
        {
          "id": 43746029,
          "author": "bsder",
          "text": "&gt; The zig community bewilders me at times with their love for lashing themselves. The sort of discussions which new sort of self-harm they&#x27;d love to enforce on everybody is borderline disturbing. Personally, I find the idea that a <i>compiler</i> might be able to reach outside itself completely terrifying (Access the network or a database? Are you nuts?). That should be 100% the job of a build system. Now, you can certainly argue that generating a text file may or may not be the best way to reify the result back into the compiler.  However, what the compiler gets and generates should be completely deterministic.",
          "depth": 1,
          "sentiment": 0.12088744588744589
        },
        {
          "id": 43746553,
          "author": "ephaeton",
          "text": "&gt; Personally, I find the idea that a compiler might be able to reach outside itself completely terrifying (Access the network or a database? Are you nuts?). What is &quot;itself&quot; here, please? Access a static &#x27;external&#x27; source? Access a dynamically generated &#x27;external&#x27; source? If that file is generated in the build system &#x2F; build process as derived information, would you put it under version control? If not, are you as nuts as I am? Some processes require sharp tools, and you can&#x27;t always be afraid to handle one. If all you have is a blunt tool, well, you know how the saying goes for C++. &gt; However, what the compiler gets and generates should be completely deterministic. The zig community treats &#x27;zig build&#x27; as &quot;the compile step&quot;, ergo what &quot;the compiler&quot; gets ultimately is decided &quot;at compile, er, zig build time&quot;. What the compiler gets, i.e., what zig build generates within the same user-facing process, is not deterministic. Why would it be. Generating an interface is something that you want to be part of a streamline process. Appeasing C interfaces will be moving to a zig build-time multi-step process involving zig&#x27;s &#x27;translate-c&#x27; whose output you then import into your zig file. You think anybody is going to treat that output differently than from what you&#x27;d get from doing this invisibly at comptime (which, btw, is what practically happens now)?",
          "depth": 2,
          "sentiment": -0.056818181818181816
        },
        {
          "id": 43747717,
          "author": "bsder",
          "text": "&gt; The zig community treats &#x27;zig build&#x27; as &quot;the compile step&quot;, ergo what &quot;the compiler&quot; gets ultimately is decided &quot;at compile, er, zig build time&quot;. What the compiler gets, i.e., what zig build generates within the same user-facing process, is not deterministic. I know of no build system that is completely deterministic unless you go through the process of <i>very</i> explicitly pinning things.  Whereas practically <i>every</i> compiler is deterministic (gcc, for example, would rebuild itself 3 times and compare the last two to make sure they were byte identical).  Perhaps there needs to be &quot;zigmeson&quot; (work out and generate dependencies) and &quot;zigninja&quot; (just call compiler on static resources) to set things apart, but it doesn&#x27;t change the fact that &quot;zig build&quot; dispatches to a &quot;build system&quot; and &quot;zig&quot;&#x2F;&quot;zig cc&quot; dispatches to a &quot;compiler&quot;. &gt; Appeasing C interfaces will be moving to a zig build-time multi-step process involving zig&#x27;s &#x27;translate-c&#x27; whose output you then import into your zig file. You think anybody is going to treat that output differently than from what you&#x27;d get from doing this invisibly at comptime (which, btw, is what practically happens now)? That&#x27;s a completely different issue, but it illustrates the problem <i>perfectly</i>. The problem is that @cImport() can be called from two different modules on the same file.  What about if there are three?  What about if they need different versions?  What happens when a previous @cImport modifies how that file translates.  How do you do link time optimization on that? This is <i>exactly</i> why your compiler needs to run on static resources that have already been resolved.  I&#x27;m fine with my build system calling a SAT solver to work out a Gordian Knot of dependencies.  I am <i>not</i> fine with my compiler needing to do that resolution.",
          "depth": 3,
          "sentiment": 0.15111111111111114
        },
        {
          "id": 43748448,
          "author": "eddythompson80",
          "text": "&gt; Personally, I find the idea that a compiler might be able to reach outside itself completely terrifying (Access the network or a database? Are you nuts?). Why though? F# has this feature called TypeProviders where you can emit types to the compiler. For example, you can do do: <pre><code>   type DbSchema = PostgresTypeProvider&lt;&quot;postgresql:&#x2F;&#x2F;postgres:...&quot;&gt;\n   type WikipediaArticle = WikipediaTypeProvider&lt;&quot;https:&#x2F;&#x2F;wikipedia.org&#x2F;wiki&#x2F;Hello&quot;&gt;\n\n</code></pre>\nand now you have a type that references that Article or that DB. You can treat it as if you had manually written all those types. You can fully inspect it in the IDE, debugger or logger. It&#x27;s a full type that&#x27;s autogenerated in a temp directory. When I first saw it, I thought it was really strange. Then thought about it abit, played with it, and thought it was brilliant. Literally one of the smartest ideas ever. It&#x27;s first class codegen framework. There were some limitations, but still. After using it in a real project, you figure out why it didn&#x27;t catch on. It&#x27;s so close, but it&#x27;s missing something. Just one thing is out of place there. The interaction is painful for anything that&#x27;s not a file source, like CsvTypeProvider or a public internet url. It does also create this odd dependenciey that your code has that can&#x27;t be source controlled or reproduced. There were hacks and workarounds, but nothing felt right for me. It was however, the best attempt at a statically typed language trying to imitate python or javascript scripting syntax. Where you just say put a db uri, and you start assuming types.",
          "depth": 2,
          "sentiment": 0.10119047619047619
        },
        {
          "id": 43747061,
          "author": "panzi",
          "text": "&gt; Personally, I find the idea that a compiler might be able to reach outside itself completely terrifying (Access the network or a database? Are you nuts?). Yeah, although so can build.rs or whatever you call in your Makefile. If something like cargo would have built-in sandboxing, that would be interesting.",
          "depth": 2,
          "sentiment": 0.0
        },
        {
          "id": 43746364,
          "author": "bmacho",
          "text": "They are not advocating for IO in the compiler, but everything else that other languages can do with macros: run commands comptime, generate code, read code, modify code. It&#x27;s proven to be very useful.",
          "depth": 2,
          "sentiment": 0.1325
        },
        {
          "id": 43747577,
          "author": "bsder",
          "text": "I&#x27;m going to make you defend that statement that they are &quot;useful&quot;.  I would counter than macros are &quot;powerful&quot;. However, &quot;macros&quot; are a disaster to debug in every language that they appear.  &quot;comptime&quot; sidesteps that because you can generally force it to run at runtime where your normal debugging mechanisms work just fine (returning a type being an exception). &quot;Macros&quot; generally impose extremely large cognitive overhead and making them hygienic has spawned the careers of countless CS professors.  In addition, macros often impose significant <i>compiler</i> overhead (how many crates do Rust&#x27;s proc-macros pull in?). It is not at all clear that the full power of general macros is worth the downstream grief that they cause (I also hold this position for a lot of compiler optimizations, but that&#x27;s a rant for a different day).",
          "depth": 3,
          "sentiment": 0.1254251700680272
        },
        {
          "id": 43747350,
          "author": "forrestthewoods",
          "text": "&gt; Personally, I find the idea that a compiler might be able to reach outside itself completely terrifying (Access the network or a database? Are you nuts?). It\u2019s not the compiler per se. Let\u2019s say you want a build system that is capable of generating code. Ok we can all agree that\u2019s super common and not crazy. Wouldn\u2019t it be great if the code that generated Zig code also be written in Zig? Why should codegen code be written in some completely unrelated language? Why should developers have to learn a brand new language to do compile time code Gen? Why yes Rust macros I\u2019m staring angrily at you!",
          "depth": 2,
          "sentiment": 0.07266899766899768
        },
        {
          "id": 43745717,
          "author": "User23",
          "text": "Learning XS (maybe with Swig?) was a great way to actually understand Perl.",
          "depth": 1,
          "sentiment": 0.4
        }
      ],
      "avg_sentiment": 0.11270155241583812
    },
    {
      "block": [
        {
          "id": 43744877,
          "author": "pyrolistical",
          "text": "What makes comptime really interesting is how fluid it is as you work. At some point you realize you need type information, so you just add it to your func params. That bubbles all the way up and you are done. Or you realize in certain situation it is not possible to provide the type and you need to solve a arch&#x2F;design issue.",
          "depth": 0,
          "sentiment": 0.17857142857142858
        },
        {
          "id": 43745610,
          "author": "Zambyte",
          "text": "If the type that you&#x27;re passing as an argument is the type of another argument, you can keep the API simpler by just using @TypeOf(arg) internally in the function instead.",
          "depth": 1,
          "sentiment": 0.0
        }
      ],
      "avg_sentiment": 0.08928571428571429
    },
    {
      "block": [
        {
          "id": 43745047,
          "author": "karmakaze",
          "text": "&gt; Zig\u2019s comptime feature is most famous for what it can do: generics!, conditional compilation!, subtyping!, serialization!, ORM! That\u2019s fascinating, but, to be fair, there\u2019s a bunch of languages with quite powerful compile time evaluation capabilities that can do equivalent things. I&#x27;m curious what are these other languages that can do these things? I read HN regularly but don&#x27;t recall them. Or maybe that&#x27;s including things like Java&#x27;s annotation processing which is so clunky that I wouldn&#x27;t classify them to be equivalent.",
          "depth": 0,
          "sentiment": 0.371875
        },
        {
          "id": 43745506,
          "author": "foobazgt",
          "text": "Yeah, I&#x27;m not a big fan of annotation processing either. It&#x27;s simultaneously heavyweight and unwieldy, and yet doesn&#x27;t do enough. You get all the annoyance of working with a full-blown AST, and none of the power that comes with being able to manipulate an AST. Annotations themselves are pretty great, and AFAIK, they are most widely used with reflection or bytecode rewriting instead. I get that the maintainers dislike macro-like capabilities, but the reality is that many of the nice libraries&#x2F;facilities Java has (e.g. transparent spans), just aren&#x27;t possible without AST-like modifications. So, the maintainers don&#x27;t provide 1st class support for rewriting, and they hold their noses as popular libraries do it. Closely related, I&#x27;m pretty excited to muck with the new class file API that just went GA in 24 (<a href=\"https:&#x2F;&#x2F;openjdk.org&#x2F;jeps&#x2F;484\" rel=\"nofollow\">https:&#x2F;&#x2F;openjdk.org&#x2F;jeps&#x2F;484</a>). I don&#x27;t have experience with it yet, but I have high hopes.",
          "depth": 1,
          "sentiment": 0.2857102272727273
        },
        {
          "id": 43746810,
          "author": "pron",
          "text": "Java&#x27;s annotation processing is intentionally limited so that compiling with them cannot change the semantics of the Java language as defined by the Java Language Specification (JLS). Note that more intrusive changes -- including not only bytecode-rewriting agents, but also the use of those AST-modifying &quot;libraries&quot; (really, languages) -- require command-line flags that tell you that the semantics of code may be impacted by some other code that is identified in those flags. This is part of &quot;integrity by default&quot;: <a href=\"https:&#x2F;&#x2F;openjdk.org&#x2F;jeps&#x2F;8305968\" rel=\"nofollow\">https:&#x2F;&#x2F;openjdk.org&#x2F;jeps&#x2F;8305968</a>",
          "depth": 2,
          "sentiment": 0.10071428571428571
        },
        {
          "id": 43747870,
          "author": "foobazgt",
          "text": "Just because something mucks with a program&#x27;s AST doesn&#x27;t mean that it&#x27;s introducing a new &quot;language&quot;. You wouldn&#x27;t call using reflection, &quot;creating a new language&quot;, either, and many of these libraries can be implemented either way. (Usually a choice between adding an additional build step, runtime overhead, and ease of implementation). It just really depends upon the details of the transform. The integrity by default JEPs are really about trying to reduce developers depending upon JDK&#x2F;JRE implementation details, for example, sun.misc.Unsafe. From the JEP: &quot;In short: The use of JDK-internal APIs caused serious migration issues, there was no practical mechanism that enabled robust security in the current landscape, and new requirements could not be met. Despite the value that the unsafe APIs offer to libraries, frameworks, and tools, the ongoing lack of integrity is untenable. Strong encapsulation and the restriction of the unsafe APIs \u2014 by default \u2014 are the solution.&quot; If you&#x27;re dependent on something like ClassFileTransformer, -javaagent, or setAccessible, you&#x27;ll just set a command-line flag. If you&#x27;re not, it&#x27;s because you&#x27;re already doing this through other means like a custom ClassLoader or a build step.",
          "depth": 3,
          "sentiment": 0.055506993006993015
        },
        {
          "id": 43745080,
          "author": "awestroke",
          "text": "Rust, D, Nim, Crystal, Julia",
          "depth": 1,
          "sentiment": 0.0
        },
        {
          "id": 43745645,
          "author": "elcritch",
          "text": "Definitely, you can do most of those things in Nim without macros using templates and compile time stuff. It\u2019s preferable to macros when possible. Julia has fantastic compile time abilities as well. It\u2019s beautiful to implement an incredibly fast serde in like 10 lines without requiring other devs to annotate their packages. I wouldn\u2019t include Rust on that list if we\u2019re speaking of compile time and compile time type abilities. Last time I tried it Rust\u2019s const expression system is pretty limited. Rust\u2019s macro system likewise is also very weak. Primarily you can only get type info by directly passing the type definition to a macro, which is how derive and all work.",
          "depth": 2,
          "sentiment": 0.14400510204081635
        },
        {
          "id": 43746822,
          "author": "tialaramex",
          "text": "Rust has <i>two</i> macro systems, the proc macros are allowed to do absolutely whatever they please because they&#x27;re actually executing in the compiler. Now, <i>should</i> they do anything they please? Definitely not, but they can. That&#x27;s why there&#x27;s a (serious) macro which runs your Python code, and a (joke, in the sense that you should never use it, not that it wouldn&#x27;t work) macro which replaces your running compiler with a different one so that code which is otherwise invalid will compile anyway...",
          "depth": 3,
          "sentiment": -0.02666666666666666
        },
        {
          "id": 43746836,
          "author": "int_19h",
          "text": "&gt; Rust\u2019s macro system likewise is also very weak. How so? Rust procedural macros operate on token stream level while being able to tap into the parser, so I struggle to think of what they <i>can&#x27;t</i> do, aside from limitations on the syntax of the macro.",
          "depth": 3,
          "sentiment": 0.006249999999999978
        },
        {
          "id": 43747359,
          "author": "forrestthewoods",
          "text": "Rust macros are a mutant foreign language. A much much better system would be one that lets you write vanilla Rust code to manipulate either the token stream or the parsed AST.",
          "depth": 4,
          "sentiment": 0.1875
        },
        {
          "id": 43748134,
          "author": "dwattttt",
          "text": "...? Proc macros _are_ vanilla Rust code written to manipulate a token stream.",
          "depth": 5,
          "sentiment": 0.0
        },
        {
          "id": 43747055,
          "author": "Nullabillity",
          "text": "Rust macros don&#x27;t really understand the types involved. If you have a derive macro for <pre><code>    #[derive(MyTrait)]\n    struct Foo {\n        bar: Bar,\n        baz: Baz,\n    }\n</code></pre>\nthen your macro <i>can</i> see that it references Bar and Baz, but it can&#x27;t know <i>anything</i> about how those types are defined. Usually, the way to get around it is to define some trait on both Bar and Baz, which your Foo struct depends on, but that still only gives you access to that information at runtime, not when evaluating your macro. Another case would be something like <pre><code>    #[my_macro]\n    fn do_stuff() -&gt; Bar {\n        let x = foo();\n        x.bar()\n    }\n</code></pre>\nYour macro would be able to see that you call the functions foo() and Something::bar(), but it wouldn&#x27;t have the context to know the type of x. And even if you did have the context to be able to see the scope, you probably still aren&#x27;t going to reimplement rustc&#x27;s type inference rules just for your one macro. Scala (for example) is different: any AST node is tagged with its corresponding type that you can just ask for, along with any context to expand on that (what fields does it have? does it implement this supertype? are there any relevant implicit conversions in scope?). There are both up- and downsides to that (personally, I do quite like the locality that Rust macros enforce, for example), but Rust macros are unquestionably <i>weaker</i>.",
          "depth": 4,
          "sentiment": 0.16875
        },
        {
          "id": 43748406,
          "author": "elcritch",
          "text": "Thanks, that\u2019s exactly what I was referencing. In lisp the type doesn\u2019t matter as much, just the structure, as maps or other dynamic pieces will be used. However in typed languages it matters a lot.",
          "depth": 5,
          "sentiment": 0.10500000000000001
        },
        {
          "id": 43746241,
          "author": "rurban",
          "text": "Perl BEGIN blocks",
          "depth": 2,
          "sentiment": 0.0
        },
        {
          "id": 43747259,
          "author": "tmtvl",
          "text": "PPR + keyword::declare (shame that Damien didn&#x27;t actually call it keyword::keyword).",
          "depth": 3,
          "sentiment": 0.0
        },
        {
          "id": 43745688,
          "author": "ephaeton",
          "text": "well, the lisp family of languages surely can do all of that, and more. Check out, for example, clojure&#x27;s version of zig&#x27;s dropped &#x27;async&#x27;. It&#x27;s a macro.",
          "depth": 1,
          "sentiment": 0.5
        }
      ],
      "avg_sentiment": 0.12657632942454372
    },
    {
      "block": [
        {
          "id": 43744951,
          "author": "ashvardanian",
          "text": "Previous submission: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43738703\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43738703</a>",
          "depth": 0,
          "sentiment": -0.16666666666666666
        }
      ],
      "avg_sentiment": -0.16666666666666666
    },
    {
      "block": [
        {
          "id": 43744932,
          "author": "no_wizard",
          "text": "I like the Zig language and tooling. I do wish there was a safety mode that give the same guarantees as Rust, but it\u2019s a huge step above C&#x2F;C++. I am also extremely impressed with the Zig compiler. Perhaps the safety is the tradeoff with the comparative ease of using the language compared to Rust, but I\u2019d love the best of both worlds if it were possible",
          "depth": 0,
          "sentiment": 0.41428571428571426
        },
        {
          "id": 43745418,
          "author": "ksec",
          "text": "&gt;but I\u2019d love the best of both worlds if it were possible I am just going to quote what pcwalton said the other day that perhaps answer your question. &gt;&gt; I\u2019d be much more excited about that promise [memory safety in Rust] if the compiler provided that safety, rather than asking the programmer to do an extraordinary amount of extra work to conform to syntactically enforced safety rules. Put the complexity in the compiler, dudes. &gt; That exists; it&#x27;s called garbage collection. &gt;If you don&#x27;t want the performance characteristics of garbage collection, something has to give. Either you sacrifice memory safety or you accept a more restrictive paradigm than GC&#x27;d languages give you. For some reason, programming language enthusiasts think that if you think really hard, every issue has some solution out there without any drawbacks at all just waiting to be found. But in fact, creating a system that has zero runtime overhead and unlimited aliasing with a mutable heap is as impossible as <i>finding two even numbers whose sum is odd.</i> [1] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43726315\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43726315</a>",
          "depth": 1,
          "sentiment": 0.19318181818181823
        },
        {
          "id": 43745760,
          "author": "the__alchemist",
          "text": "Maybe this is a bad place to ask, but: Those experienced in manual-memory langs: What in particular do you find cumbersome about the borrow system? I&#x27;ve hit some annoyances like when splitting up struct fields into params where more than one is mutable, but that&#x27;s the only friction point that comes to mind. I ask because I am obvious blind to other cases - that&#x27;s what I&#x27;m curious about! I generally find the &amp;s to be a net help even without mem safety ... They make it easier to reason about structure, and when things mutate.",
          "depth": 2,
          "sentiment": 0.006060606060606077
        },
        {
          "id": 43745963,
          "author": "sgeisenh",
          "text": "Lifetime annotations can be burdensome when trying to avoid extraneous copies and they feel contagious (when you add a lifetime annotation to a frequently used type, it bubbles out to anything that uses that type unless you&#x27;re willing to use unsafe to extend lifetimes). The solutions to this problem (tracking indices instead of references) lose a lot of benefits that the borrow checker provides. The aliasing rules in Rust are also pretty strict. There are plenty of single-threaded programs where I want to be able to occasionally read a piece of information through an immutable reference, but that information can be modified by a different piece of code. This usually indicates a design issue in your program but sometimes you just want to throw together some code to solve an immediate problem. The extra friction from the borrow checker makes it less attractive to use Rust for these kinds of programs.",
          "depth": 3,
          "sentiment": 0.14833333333333334
        },
        {
          "id": 43746402,
          "author": "bogdanoff_2",
          "text": "&gt;There are plenty of single-threaded programs where I want to be able to occasionally read a piece of information through an immutable reference, but that information can be modified by a different piece of code. You could do that using Cell or RefCell. I agree that it makes it more cumbersome.",
          "depth": 4,
          "sentiment": 0.25
        },
        {
          "id": 43747347,
          "author": "dzaima",
          "text": "I imagine a large part is just how one is used to doing stuff. Not being forced to be explicit about mutability and lifetimes allows a bunch of neat stuff that does not translate well to Rust, even if the desired thing in question might not be hard to do in another way. (but that other way might involve more copies &#x2F; indirections, which users of manually-memory langs would (perhaps rightfully, perhaps pointlessly) desire to avoid if possible, but Rust users might just be comfortable with) This separation is also why it is basically impossible to make apples-to-apples comparisons between languages. Messy things I&#x27;ve hit (from ~5KLoC of Rust; I&#x27;m a Rust beginner, I primarily do C) are: cyclical references; a large structure that needs efficient single-threaded mutation while referenced from multiple places (i.e. must use some form of cell) at first, but needs to be sharable multithreaded after all the mutating is done; self-referential structures are roughly impossible to move around (namely, an object holding &amp;-s to objects allocated by a bump allocator, movable around as a pair, but that&#x27;s not a thing (without libraries that I couldn&#x27;t figure out at least)); and refactoring mutability&#x2F;lifetimes is also rather messy.",
          "depth": 3,
          "sentiment": -0.06008403361344537
        },
        {
          "id": 43745891,
          "author": "rc00",
          "text": "&gt; What in particular do you find cumbersome about the borrow system? The refusal to accept code that the developer knows is correct, simply because it does not fit how the borrow checker wants to see it implemented. That kind of heavy-handed and opinionated supervision is overhead to productivity. (In recent times, others have taken to saying that Rust is less &quot;fun.&quot;) When the purpose of writing code is to solve a problem and not engage in some pedantic or academic exercise, there are much better tools for the job. There are also times when memory safety is not a paramount concern. That makes the overhead of Rust not only unnecessary but also unwelcome.",
          "depth": 3,
          "sentiment": 0.07916666666666668
        },
        {
          "id": 43747477,
          "author": "charlotte-fyi",
          "text": "Isn&#x27;t the persistent failure of developers to &quot;know&quot; that their code is correct the entire point? Unless you have mechanical proof, in the aggregate and working on any project of non-trivial size &quot;knowing&quot; is really just &quot;assuming.&quot; This isn&#x27;t academic or pedantic, it&#x27;s a basic epistemological claim with regard to what writing software actually looks like in practice. You, in fact, do not know, and your insistence that you do is precisely the reason that you are at greater risk of creating memory safety vulnerabilities.",
          "depth": 4,
          "sentiment": 0.09791666666666668
        },
        {
          "id": 43745915,
          "author": "the__alchemist",
          "text": "Thank you for the answer! Do you have an example? I&#x27;m having a fish-doesn&#x27;t-know-water problem.",
          "depth": 4,
          "sentiment": 0.0
        },
        {
          "id": 43746854,
          "author": "int_19h",
          "text": "Basically anything that involves objects mutually referencing each other.",
          "depth": 5,
          "sentiment": -0.125
        },
        {
          "id": 43746946,
          "author": "the__alchemist",
          "text": "Oh, that does sound tough in rust! I&#x27;m not even sure how to approach it; good to know it&#x27;s a useful pattern in other langs.",
          "depth": 6,
          "sentiment": 0.21481481481481482
        },
        {
          "id": 43747522,
          "author": "int_19h",
          "text": "Well, one can always write unsafe Rust. Although the more usual pattern here is to ditch pointers and instead have a giant array of objects referring to each other via indices into said array. But this is effectively working around the borrow checker - those indices are semantically unchecked references, and although out-of-bounds checks will prevent memory corruption, it is possible to store index to some object only for that object to be replaced with something else entirely later.",
          "depth": 7,
          "sentiment": 0.090625
        },
        {
          "id": 43748790,
          "author": "Cloudef",
          "text": "&gt;unsafe rust\nWhich is worse than C",
          "depth": 8,
          "sentiment": -0.4
        },
        {
          "id": 43747732,
          "author": "estebank",
          "text": "&gt; it is possible to store index to some object only for that object to be replaced with something else entirely later. That&#x27;s what generational arenas are for, at the cost of having to check for index validity on every access. But that cost is only in comparison to &quot;keep a pointer in a field&quot; with no additional logic, which is bug-prone.",
          "depth": 8,
          "sentiment": 0.0
        },
        {
          "id": 43746004,
          "author": "Ygg2",
          "text": "&gt; The refusal to accept code that the developer knows is correct, How do you know it is correct? Did you prove it with pre-condition, invariants and post-condition? Or did you assume based on prior experience.",
          "depth": 4,
          "sentiment": 0.0
        },
        {
          "id": 43746298,
          "author": "edflsafoiewq",
          "text": "One example is a function call that doesn&#x27;t compile, but will if you inline the function body. Compilation is prevented only by the insufficient expressiveness of the function signature.",
          "depth": 5,
          "sentiment": 0.0
        },
        {
          "id": 43746128,
          "author": "yohannesk",
          "text": "Writing correct code did not start after the introduction of the rust programming language",
          "depth": 5,
          "sentiment": 0.0
        },
        {
          "id": 43746376,
          "author": "Ygg2",
          "text": "Nope, but claims of knowing to write correct code (especially C code) without borrow checker sure did spike with its introduction. Hence, my question. How do you know you haven&#x27;t been writing unsafe code for years, when C unsafe guidelines have like 200 entries[1]. [1]<a href=\"https:&#x2F;&#x2F;www.dii.uchile.cl&#x2F;~daespino&#x2F;files&#x2F;Iso_C_1999_definition.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.dii.uchile.cl&#x2F;~daespino&#x2F;files&#x2F;Iso_C_1999_definit...</a> (Annex J.2 page 490)",
          "depth": 6,
          "sentiment": 0.25
        },
        {
          "id": 43746850,
          "author": "int_19h",
          "text": "It&#x27;s not difficult to write a provably correct implementation of doubly linked list in C, but it is very painful to do in Rust because the borrow checker really hates this kind of mutually referential objects.",
          "depth": 7,
          "sentiment": 0.028000000000000014
        },
        {
          "id": 43746263,
          "author": "Starlevel004",
          "text": "Lifetimes add an impending sense of doom to writing any sort of deeply nested code. You get this deep without writing a lifetime... uh oh, this struct needs a reference, and now you need to add a generic parameter to everything everywhere you&#x27;ve ever written and it feels miserable. Doubly so when you&#x27;ve accidentally omitted a lifetime generic somewhere and it compiles now but then you do some refactoring and it <i>won&#x27;t</i> work anymore and you need to go back and re-add the generic parameter everywhere.",
          "depth": 3,
          "sentiment": -0.125
        },
        {
          "id": 43746586,
          "author": "pornel",
          "text": "There is a stark contrast in usability of self-contained&#x2F;owning types vs types that are temporary views bound by a lifetime of the place they are borrowing from. But this is an inherent problem for all non-GC languages that allow saving pointers to data on the stack (Rust doesn&#x27;t need lifetimes for by-reference heap types). In languages without lifetimes you just don&#x27;t get any compiler help in finding places that may be affected by dangling pointers. This is similar to creating a broadly-used data structure and realizing that some field has to be optional. Option&lt;T&gt; will require you to change everything touching it, and virally spread through all the code that wanted to use that field unconditionally. However, that&#x27;s not the fault of the Option syntax, it&#x27;s the fault of semantics of optionality. In languages that don&#x27;t make this &quot;miserable&quot; at compile time, this problem manifests with a whack-a-mole of NullPointerExceptions at run time. With experience, I don&#x27;t get this &quot;oh no, now there&#x27;s a lifetime popping up everywhere&quot; surprise in Rust any more. Whether something is going to be a temporary view or permanent storage can be known ahead of time, and if it can be both, it can be designed with Cow-like types. I also got a sense for when using a temporary loan is a premature optimization. All data has to be stored somewhere (you can&#x27;t have a reference to data that hasn&#x27;t been stored). Designs that try to be ultra-efficient by allowing only temporary references often force data to be stored in a temporary location first, and then borrowed, which doesn&#x27;t avoid any allocations, only adds dependencies on external storage. Instead, the design can support moving or collecting data into owned (non-temporary) storage directly. It can then keep it for an arbirary lifetime without lifetime annotations, and hand out temporary references to it whenever needed. The run-time cost can be the same, but the semantics are much easier to work with.",
          "depth": 4,
          "sentiment": 0.12272727272727274
        },
        {
          "id": 43746507,
          "author": "the__alchemist",
          "text": "I guess the dodge on this one is not using refs in structs. This opens you up to index errors though because it presumably means indexing arrays etc. Is this the tradeoff. (I write loads of rusts in a variety of domains, and rarely need a manual lifetime)",
          "depth": 4,
          "sentiment": 0.3
        },
        {
          "id": 43747105,
          "author": "quotemstr",
          "text": "And those index values are just pointers by another name!",
          "depth": 5,
          "sentiment": 0.0
        },
        {
          "id": 43747764,
          "author": "estebank",
          "text": "It&#x27;s not &quot;just pointers&quot;, because they can have additional semantics and assurances beyond &quot;give me the bits at this address&quot;. The index value can be tied to a specific container (using new types for indexing so tha you can&#x27;t make the mistake of getting value 1 from container A when it represents an index from container B), can prevent use after free (by embedding data about the value&#x27;s &quot;generation&quot; in the key), and makes the index resistant to relocation of the values (because of the additional level of indirection of the index to the value&#x27;s location).",
          "depth": 6,
          "sentiment": 0.1340909090909091
        },
        {
          "id": 43748492,
          "author": "quotemstr",
          "text": "Yes, but like raw pointers, they lack lifetime guarantees and invite use after free vulnerabilities",
          "depth": 7,
          "sentiment": 0.08461538461538462
        },
        {
          "id": 43745462,
          "author": "skybrian",
          "text": "Yes, but I\u2019m not hoping for that. I\u2019m hoping for something like a scripting language with simpler lifetime annotations. Is Rust going to be the last popular language to be invented that explores that space? I hope not.",
          "depth": 2,
          "sentiment": 0.3
        },
        {
          "id": 43745665,
          "author": "hyperbrainer",
          "text": "I was quite impressed with Austral[0], which used Linear Types and avoids the whole Rust-like implementation in favour of a more easily understandable system, albeit slightly more verbose. [0]<a href=\"https:&#x2F;&#x2F;borretti.me&#x2F;article&#x2F;introducing-austral\" rel=\"nofollow\">https:&#x2F;&#x2F;borretti.me&#x2F;article&#x2F;introducing-austral</a>",
          "depth": 3,
          "sentiment": 0.5266666666666666
        },
        {
          "id": 43746855,
          "author": "renox",
          "text": "Austra&#x27;s concept are interesting but the introduction doesn&#x27;t show how to handle correctly errors in this language..",
          "depth": 4,
          "sentiment": 0.5
        },
        {
          "id": 43748794,
          "author": "hyperbrainer",
          "text": "Austral&#x27;s specification is one of the most beautiful and well-written pieces of documentation I have ever found. It&#x27;s section on error handling in Austral[0] cover everything from rationale and alternatives to concrete examples of how exceptions should be handled in conjunction with linear types. <a href=\"https:&#x2F;&#x2F;austral-lang.org&#x2F;spec&#x2F;spec.html#rationale-errors\" rel=\"nofollow\">https:&#x2F;&#x2F;austral-lang.org&#x2F;spec&#x2F;spec.html#rationale-errors</a>",
          "depth": 5,
          "sentiment": 0.5
        },
        {
          "id": 43745858,
          "author": "Philpax",
          "text": "You may be interested in <a href=\"https:&#x2F;&#x2F;dada-lang.org&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;dada-lang.org&#x2F;</a>, which is not ready for public consumption, but is a language by one of Rust&#x27;s designers that aims to be higher-level while still keeping much of the goodness from Rust.",
          "depth": 3,
          "sentiment": 0.0875
        },
        {
          "id": 43745902,
          "author": "skybrian",
          "text": "The first and last blog post was in 2021. Looks like it\u2019s still active on Github, though?",
          "depth": 4,
          "sentiment": 0.03888888888888889
        },
        {
          "id": 43745947,
          "author": "Ygg2",
          "text": "&gt; Is Rust going to be the last popular language to be invented that explores that space? I hope not. Seeing how most people hate the lifetime annotations, yes. For the foreseeable future. People want unlimited freedom. Unlimited freedom rhymes with unlimited footguns.",
          "depth": 3,
          "sentiment": 0.06000000000000001
        },
        {
          "id": 43746350,
          "author": "xmorse",
          "text": "There is Mojo and Vale (which was created by a now Mojo core contributor)",
          "depth": 4,
          "sentiment": 0.0
        },
        {
          "id": 43746930,
          "author": "no_wizard",
          "text": "I have zero issue with needing runtime GC or equivalent like ARC. My issue is with ergonomics and performance. In my experience with a range of languages, the most performant way of writing the code is not the way you would idiomatically write it. They make good performance more complicated than it should be. This holds true to me for my work with Java, Python, C# and JavaScript. What I suppose I\u2019m looking for is a better compromise between having some form of managed runtime vs non managed And yes, I\u2019ve also tried Go, and it\u2019s DX is its own type of pain for me. I should try it again now that it has generics",
          "depth": 2,
          "sentiment": 0.37857142857142856
        },
        {
          "id": 43747394,
          "author": "neonsunset",
          "text": "Using spans, structs, object and array pools is considered fairly idiomatic C# if you care about performance (and many methods now default to just spans even outside that). What kind of idiomatic or unidiomatic C# do you have in mind? I\u2019d say if you are okay with GC side effects, achieving good performance targets is way easier than if you care about P99&#x2F;999.",
          "depth": 3,
          "sentiment": 0.5
        },
        {
          "id": 43745791,
          "author": "spullara",
          "text": "With Java ZGC the performance aspect has been fixed (&lt;1ms pause times and real world throughput improvement). Memory usage though will always be strictly worse with no obvious way to improve it without sacrificing the performance gained.",
          "depth": 2,
          "sentiment": -0.024999999999999994
        },
        {
          "id": 43747793,
          "author": "estebank",
          "text": "IMO the best chance Java has to close the gap on memory utilisation is Project Valhalla[1] which brings value types to the JVM, but the specifics will matter. If it requires backwards incompatible opt-in ceremony, the adoption in the Java ecosystem is going to be an uphill battle, so the wins will remain theoretical and be unrealised. If it is transparent, then it might reduce the memory pressure of Java applications overnight. Last I heard was that the project was ongoing, but production readiness remained far in the future. I hope they pull it off. 1: <a href=\"https:&#x2F;&#x2F;openjdk.org&#x2F;projects&#x2F;valhalla&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;openjdk.org&#x2F;projects&#x2F;valhalla&#x2F;</a>",
          "depth": 3,
          "sentiment": 0.23333333333333336
        },
        {
          "id": 43748268,
          "author": "spullara",
          "text": "Agree, been waiting for it for almost a decade.",
          "depth": 4,
          "sentiment": 0.0
        },
        {
          "id": 43745201,
          "author": "xedrac",
          "text": "I like Zig as a replacement for C, but not C++ due to its lack of RAII.  Rust on the other hand is a great replacement for C++.  I see Zig as filling a small niche where allocation failures are paramount - very constrained embedded devices, etc...  Otherwise, I think you just get a lot more with Rust.",
          "depth": 1,
          "sentiment": 0.19791666666666666
        },
        {
          "id": 43745507,
          "author": "rastignack",
          "text": "Compile times and painful to refactor codebase are rust\u2019s main drawbacks for me though. It\u2019s totally subjective but I find the language boring to use. For side projects I like having fun thus I picked zig. To each his own of course.",
          "depth": 2,
          "sentiment": -0.10555555555555553
        },
        {
          "id": 43745951,
          "author": "nicce",
          "text": "&gt; refactor codebase are rust\u2019s main drawbacks Hard disagree about refactoring. Rust is one of the few languages where you can actually do refactoring rather safely without having tons of tests that just exist to catch issues if code changes.",
          "depth": 3,
          "sentiment": 0.03499999999999999
        },
        {
          "id": 43746010,
          "author": "rastignack",
          "text": "Lifetimes and generic tend to leak so you have to modify your code all around the place when you touch them though.",
          "depth": 4,
          "sentiment": 0.0
        },
        {
          "id": 43745255,
          "author": "xmorse",
          "text": "Even better than RAII would be linear types, but it would require a borrow checker to track the lifetimes of objects. Then you would get a compiler error if you forget to call a .destroy() method",
          "depth": 2,
          "sentiment": 0.5
        },
        {
          "id": 43745597,
          "author": "throwawaymaths",
          "text": "no you just need analysis with a dependent type system (which linear types are a subset of).  it doesn&#x27;t have to be in the compiler.  there was a proof of concept here a few months ago: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42923829\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42923829</a> <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43199265\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43199265</a>",
          "depth": 3,
          "sentiment": -0.2
        },
        {
          "id": 43745573,
          "author": "throwawaymaths",
          "text": "in principle it should be doable, possibly not in the language&#x2F;compiler itself, there was this POC a few months ago: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ityonemo&#x2F;clr\">https:&#x2F;&#x2F;github.com&#x2F;ityonemo&#x2F;clr</a>",
          "depth": 1,
          "sentiment": -0.1
        },
        {
          "id": 43744960,
          "author": "hermanradtke",
          "text": "I wish for \u201cstrict\u201d mode as well. My current thinking: TypeScript is to JavaScript as Zig is to C I am a huge TS fan.",
          "depth": 1,
          "sentiment": 0.19999999999999998
        },
        {
          "id": 43745224,
          "author": "rc00",
          "text": "Is Zig aiming to extend C or extinguish it? The embrace story is well-established at this point but the remainder is often unclear in the messaging from the community.",
          "depth": 2,
          "sentiment": 0.0
        },
        {
          "id": 43745808,
          "author": "PaulRobinson",
          "text": "It&#x27;s improved C. C interop is very important, and very valuable. However, by removing undefined behaviours, replacing macros that do weird things with well thought-through comptime, and making sure that the zig compiler is also a c compiler, you get a nice balance across lots of factors. It&#x27;s a great language, I encourage people to dig into it.",
          "depth": 3,
          "sentiment": 0.35333333333333333
        },
        {
          "id": 43746440,
          "author": "yellowapple",
          "text": "The goal rather explicitly seems to be to extinguish it - the idea being that if you&#x27;ve got Zig, there should be no reason to need to write new code in C, because literally anything possible in C should be possible (and ideally done better) in Zig. Whether that ends up happening is obviously yet to be seen; as it stands there are plenty of Zig codebases with C in the mix.  The idea, though, is that there shouldn&#x27;t be anything stopping a programmer from replacing that C with Zig, and the two languages only coexist for the purpose of allowing that replacement to be gradual.",
          "depth": 3,
          "sentiment": 0.21948051948051947
        },
        {
          "id": 43745439,
          "author": "dooglius",
          "text": "Zig is open source, so the analogy to Microsoft&#x27;s EEE [0] seems misplaced. [0] <a href=\"https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Embrace,_extend,_and_extinguish?useskin=vector\" rel=\"nofollow\">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Embrace,_extend,_and_extingu...</a>",
          "depth": 3,
          "sentiment": -0.1
        },
        {
          "id": 43745514,
          "author": "rc00",
          "text": "Open source or not isn&#x27;t the point. The point is the mission and the ecosystem. Some of the Zig proponents laud the C compatibility. Others are seeking out the &quot;pure Zig&quot; ecosystem. Curious onlookers want to know if the Zig ecosystem and community will be as hostile to the decades of C libraries as the Rust zealots have been. To be fair, I don&#x27;t believe there is a centralized and stated mission with Zig but it does feel like the story has moved beyond the &quot;Incrementally improve your C&#x2F;C++&#x2F;Zig codebase&quot; moniker.",
          "depth": 4,
          "sentiment": 0.19999999999999998
        },
        {
          "id": 43745641,
          "author": "Zambyte",
          "text": "&gt; Curious onlookers want to know if the Zig ecosystem and community will be as hostile to the decades of C libraries as the Rust zealots have been. Definitely not the case in Zig. From my experience, the relationship with C libraries amounts to &quot;if it works, use it&quot;.",
          "depth": 5,
          "sentiment": -0.05
        },
        {
          "id": 43745821,
          "author": "rc00",
          "text": "Are you referring to static linking? Dynamic linking? Importing&#x2F;inclusion? How does this translate (no pun intended) when the LLVM backend work is completed? Does this extend to reproducible builds? Hermetic builds? And the relationship with C libraries certainly feels like a placeholder, akin to before the compiler was self-hosted. While I have seen some novel projects in Zig, there are certainly more than a few &quot;pure Zig&quot; rewrites of C libraries. Ultimately, this is free will. I just wonder if the Zig community is teeing up for a repeat of Rust&#x27;s actix-web drama but rather than being because of the use of unsafe, it would be due to the use of C libraries instead of the all-Zig counterparts (assuming some level of maturity with the latter). While Zig&#x27;s community appears healthier and more pragmatic, hype and ego have a way of ruining everything.",
          "depth": 6,
          "sentiment": 0.17892857142857146
        },
        {
          "id": 43745935,
          "author": "Zambyte",
          "text": "&gt; static linking? Yes &gt; Dynamic linking? Yes &gt; Importing&#x2F;inclusion? Yes &gt; How does this translate (no pun intended) when the LLVM backend work is completed? I&#x27;m not sure what you mean. It sounds like you think they&#x27;re working on being able to use LLVM as a backend, but that has already been supported, and now they&#x27;re working on <i>not</i> depending on LLVM as a requirement. &gt; Does this extend to reproducible builds? My hunch would be yes, but I&#x27;m not certain. &gt; Hermetic builds? I have never heard of this, but I would guess the same as reproducible. &gt; While I have seen some novel projects in Zig, there are certainly more than a few &quot;pure Zig&quot; rewrites of C libraries. It&#x27;s a nice exercise, especially considering how close C and Zig are semantically. It&#x27;s helpful for learning to see how C things are done in Zig, and rewriting things lets you isolate that experience without also being troubled with creating something novel. For more than a few <i>not</i> rewrites, check out <a href=\"https:&#x2F;&#x2F;github.com&#x2F;allyourcodebase\">https:&#x2F;&#x2F;github.com&#x2F;allyourcodebase</a>, which is a group that repackages existing C libraries with the Zig package manager &#x2F; build system.",
          "depth": 7,
          "sentiment": 0.0735969387755102
        },
        {
          "id": 43745750,
          "author": "ephaeton",
          "text": "zig&#x27;s C compat is being lowered from &#x27;comptime&#x27; equivalent status to &#x27;zig build&#x27;-time equivalent status. When you&#x27;ll need to put &#x27;extern &quot;C&quot;&#x27; annotations on any import&#x2F;export to C, it&#x27;ll have gone full-circle to C++ C compat, and thus be none the wiser. andrewrk&#x27;s wording towards C and its main ecosystem (POSIX) is very hostile, if that is something you&#x27;d like to go by.",
          "depth": 5,
          "sentiment": 0.18333333333333335
        }
      ],
      "avg_sentiment": 0.11617687777731706
    },
    {
      "block": [
        {
          "id": 43745391,
          "author": "ww520",
          "text": "This is a very educational blog post. I knew \u2018comptime for\u2019 and \u2018inline for\u2019 were comptime related, but didn\u2019t know the difference. The post explains the inline version only knows the length at comptime. I guess it\u2019s for loop unrolling.",
          "depth": 0,
          "sentiment": 0.10833333333333334
        },
        {
          "id": 43746484,
          "author": "hansvm",
          "text": "The normal use case for `inline for` is when you have to close over something only known at compile time (like when iterating over the fields of a struct), but when your behavior depends on runtime information (like conditionally assigning data to those fields). Unrolling as a performance optimization is usually slightly different, typically working in batches rather than unrolling the entire thing, even when the length is known at compile time. The docs suggest not using `inline` for performance without evidence it helps in your specific usage, largely because the bloated binary is likely to be slower unless you have a good reason to believe your case is special, and also because `inline` _removes_ optimization potential from the compiler rather than adding it (its inlining passes are very, very good, and despite having an extremely good grasp on which things should be inlined I rarely outperform the compiler -- I&#x27;m never worse, but the ability to not have to even think about it unless&#x2F;until I get to the microoptimization phase of a project is liberating).",
          "depth": 1,
          "sentiment": 0.22431746031746033
        }
      ],
      "avg_sentiment": 0.16632539682539682
    },
    {
      "block": [
        {
          "id": 43745782,
          "author": "paldepind2",
          "text": "This is honestly really cool! I&#x27;ve heard praises about Zig&#x27;s comptime without really understanding what makes it tick. It initially sounds like Rust&#x27;s constant evaluation which is not particularly capable. The ability to have types represented as values at compilation time, and _only_ at compile time, is clearly very powerful. It approximates dynamic languages or run-time reflection without any of the run-time overhead and without opening the Pandora&#x27;s box that is full blown macros as in Lisp or Rust&#x27;s procedural macros.",
          "depth": 0,
          "sentiment": 0.14194444444444443
        }
      ],
      "avg_sentiment": 0.14194444444444443
    },
    {
      "block": [],
      "avg_sentiment": 0
    },
    {
      "block": [
        {
          "id": 43746616,
          "author": "forrestthewoods",
          "text": "&gt; When you execute code at compile time, on which machine does it execute? The natural answer is \u201con your machine\u201d, but it is wrong! I don\u2019t understand this. If I am cross-compiling a program is it not true that comptime code literally executes on my local host machine? Like, isn\u2019t that literally the definition of \u201ccompile-time\u201d? If there is an endian architecture change I could see Zig choosing to <i>emulate</i> the target machine on the host machine. This feels so wrong to me. HostPlatform and TargetPlatform can be different. That\u2019s fine! Hiding the host platform seems wrong. Can aomeone explain why you want to hide this seemingly critical fact? Don\u2019t get me wrong, I\u2019m 100% on board the cross-compile train. And Zig does it literally better than any other compiled language that I know. So what am I missing? Or wait. I guess the key is that, unlike Jai, comptime Zig code does NOT run at compile time. It merely refers to things that are KNOWN at compile time? Wait that\u2019s not right either. I\u2019m confused.",
          "depth": 0,
          "sentiment": -0.14982492997198876
        },
        {
          "id": 43746888,
          "author": "int_19h",
          "text": "The point is that something like sizeof(pointer) should have the same value in comptime code that it has at runtime for a given app. Which, yes, means that the comptime interpreter emulates the target machine. The reason is fairly simple: you want comptime code to be able to compute correct values for use at runtime. At the same time, there&#x27;s zero benefit to <i>not</i> hiding the host platform in comptime, because, well, what use case is there for knowing e.g. the size of pointer in the arch on which the compiler is running?",
          "depth": 1,
          "sentiment": 0.125
        },
        {
          "id": 43747015,
          "author": "forrestthewoods",
          "text": "&gt; Which, yes, means that the comptime interpreter emulates the target machine. Reasonable if that\u2019s how it works. I had absolutely no idea that Zig comptime worked this way! &gt; there&#x27;s zero benefit to not hiding the host platform in comptime I don\u2019t think this is clear. It is possibly good to hide host platform given Zig\u2019s more limited comptime capabilities. However in my $DayJob an <i>extremely</i> common and painful source of issues is trying to hide host platform when it can not in fact be hidden.",
          "depth": 2,
          "sentiment": 0.015211640211640216
        },
        {
          "id": 43747513,
          "author": "int_19h",
          "text": "Can you give an example of a use case where you <i>wouldn&#x27;t</i> want comptime behavior to match runtime, but instead expose host&#x2F;target differences?",
          "depth": 3,
          "sentiment": 0.0
        },
        {
          "id": 43747584,
          "author": "forrestthewoods",
          "text": "Let\u2019s pretend I was writing some compile-time code that generates code. For example maybe I\u2019m generating serde code. Or maybe I\u2019m generating bindings for C, Python, etc. My generation code is probably going to allocate some memory and have some pointers and do some stuff. Why on earth would I want this compile-time code to run on an emulated version of the target platform? If I\u2019m on a 64-bit platform then pointers are 8-bytes why would I pretend they aren\u2019t? Even if the target is 32-bit? Does that make sense? If the compiletime code ONLY runs on the host platform then you plausibly need to expose both host and target. I\u2019m pretty sure I\u2019m thinking about zig comptime all wrong. Something isn\u2019t clicking.",
          "depth": 4,
          "sentiment": 0.15
        }
      ],
      "avg_sentiment": 0.028077342047930286
    },
    {
      "block": [],
      "avg_sentiment": 0
    },
    {
      "block": [],
      "avg_sentiment": 0
    },
    {
      "block": [],
      "avg_sentiment": 0
    },
    {
      "block": [
        {
          "id": 43744862,
          "author": "gitroom",
          "text": "Cool!",
          "depth": 0,
          "sentiment": 0.4375
        }
      ],
      "avg_sentiment": 0.4375
    }
  ]
}